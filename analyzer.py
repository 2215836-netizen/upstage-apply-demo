from langchain_openai import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

def analyze_strategy(news_text, api_key):
    """
    Analyzes the news text using OpenAI to generate a strategic report (SWOT/PEST).
    """
    if not api_key:
        return "‚ö†Ô∏è OpenAI API Key is missing. Please enter it in the sidebar."
    
    if not news_text or len(news_text) < 50:
        return "‚ö†Ô∏è Î∂ÑÏÑùÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§."

    try:
        # Initialize LLM
        llm = ChatOpenAI(
            temperature=0.7, 
            model_name="gpt-3.5-turbo", 
            openai_api_key=api_key
        )

        template = """
        ÎãπÏã†ÏùÄ Í∏ÄÎ°úÎ≤å Î¶¨ÏÑúÏπò ÌéåÏùò ÏàòÏÑù Ï†ÑÎûµÍ∞Ä(CSO)ÏûÖÎãàÎã§.
        ÏÇºÏÑ±Í∏ÄÎ°úÎ≤åÎ¶¨ÏÑúÏπò(SGR) Í≤ΩÏòÅÏßÑÏùÑ ÏúÑÌï¥, ÏïÑÎûò Îâ¥Ïä§ ÏöîÏïΩÎ≥∏ÏùÑ Î∞îÌÉïÏúºÎ°ú Ïã¨Ï∏µÏ†ÅÏù∏ 'Ï†ÑÎûµ Ïù∏ÏÇ¨Ïù¥Ìä∏ Î¶¨Ìè¨Ìä∏'Î•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
        
        Î∞òÎìúÏãú **ÌïúÍµ≠Ïñ¥**Î°ú ÏûëÏÑ±Ìï¥Ïïº ÌïòÎ©∞, Îã§Ïùå ÌòïÏãùÏùÑ Îî∞Î•¥ÏÑ∏Ïöî:
        
        # üìë Ï†ÑÎûµÏ†Å Ïù∏ÏÇ¨Ïù¥Ìä∏ Î¶¨Ìè¨Ìä∏
        
        ## 1. Executive Summary (ÏöîÏïΩ)
        (ÌòÑÏû¨ ÏãúÏû• ÏÉÅÌô©ÏùÑ 3Î¨∏Ïû• Ïù¥ÎÇ¥Î°ú ÌïµÏã¨Îßå ÏöîÏïΩ)
        
        ## 2. Ï£ºÏöî Í∏∞Ìöå ÏöîÏù∏ (Opportunities)
        - (Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞Ïóê Í∏∞Î∞òÌïú Íµ¨Ï≤¥Ï†ÅÏù∏ Í∏∞Ìöå ÏöîÏù∏ ÎÇòÏó¥)
        
        ## 3. Ïû†Ïû¨Ï†Å ÏúÑÌòë (Threats)
        - (Í≤ΩÏüÅÏÇ¨ ÎèôÌñ•, Í∑úÏ†ú, Í∏∞Ïà†Ï†Å ÏúÑÌòë Îì±)
        
        ## 4. Ï†ÑÎûµÏ†Å Ï†úÏñ∏ (Recommendations)
        - **Îã®Í∏∞ Ï†ÑÎûµ**: (Ï¶âÏãú Ïã§Ìñâ Í∞ÄÎä•Ìïú Ï°∞Ïπò)
        - **Ïû•Í∏∞ Ï†ÑÎûµ**: (ÎØ∏Îûò Î∞©Ìñ•ÏÑ± Ï†úÏïà)
        
        ---
        **Ï∞∏Í≥† Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ (News Context)**:
        {news_content}
        """
        
        prompt = PromptTemplate(
            input_variables=["news_content"],
            template=template,
        )
        
        # In a newer LangChain version this might be LLMChain, but simplest way:
        formatted_prompt = prompt.format(news_content=news_text[:10000]) # simple truncation to avoid token limits
        response = llm.predict(formatted_prompt)
        
        return response

    except Exception as e:
        return f"Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
