from langchain_groq import ChatGroq
from langchain_core.prompts import PromptTemplate

def analyze_strategy(news_text, api_key, model_choice=None):
    """
    Analyzes the news text using Groq (Llama 3) to generate a strategic report.
    """
    if not api_key:
        return "âš ï¸ Groq API Key is missing. Please enter it in the sidebar."
    
    if not news_text or len(news_text) < 50:
        return "âš ï¸ ë¶„ì„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."

    # Model Priority List (Fallback Strategy)
    # 1. Flagship (Best Quality)
    # 2. Previous Flagship (Good Quality)
    # 3. Fast/Light (Best Availability)
    models_to_try = [
        "llama-3.3-70b-versatile", 
        "llama-3.1-70b-versatile", 
        "llama-3.1-8b-instant",
        "mixtral-8x7b-32768"
    ]

    for model_name in models_to_try:
        try:
            # Initialize LLM with current model selection
            llm = ChatGroq(
                model_name=model_name,
                groq_api_key=api_key,
                temperature=0.7
            )
            
            # Reduce context size for fallback models to save tokens/speed
            current_context_limit = 15000
            if "8b" in model_name:
                current_context_limit = 10000

            template = """
            ë‹¹ì‹ ì€ ì‚¼ì„±ê¸€ë¡œë²Œë¦¬ì„œì¹˜(SGR)ì˜ **'SGR ë“œë¦¼íŒ€'**ì…ë‹ˆë‹¤. 
            ì„¸ ëª…ì˜ ì „ë¬¸ê°€(Persona)ê°€ ëª¨ì—¬ í† ë¡ í•œ ë’¤, ìµœì¢… ì „ëµ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ì—­í• ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤.

            ---
            **[í† ë¡  ì°¸ì—¬ì]**
            1. **ğŸ‘¨â€ğŸ’¼ ê±°ì‹œê²½ì œ ì „ë¬¸ê°€ (Macro Economist)**: ê¸ˆë¦¬, í™˜ìœ¨, ì§€ì •í•™ì  ë¦¬ìŠ¤í¬, ê¸€ë¡œë²Œ ê²½ì œ íë¦„ ë¶„ì„.
            2. **ğŸ§‘â€ğŸ’» ì‚°ì—… ê¸°ìˆ  ì „ë¬¸ê°€ (Tech Specialist)**: ë°˜ë„ì²´/AI ê¸°ìˆ  íŠ¸ë Œë“œ, ê²½ìŸì‚¬(TSMC, Intel ë“±) ê¸°ìˆ  ê²©ì°¨ ë¶„ì„.
            3. **ğŸ•µï¸ ì „ëµ ì»¨ì„¤í„´íŠ¸ (Strategy Consultant)**: ìœ„ ë‘ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬, ì‚¼ì„±ì „ìê°€ ë‹¹ì¥ ì‹¤í–‰í•´ì•¼ í•  **êµ¬ì²´ì  Action Plan** ë„ì¶œ.

            ---
            **[ì…ë ¥ ë‰´ìŠ¤ ë°ì´í„°]**:
            {news_content}
            ---

            ìœ„ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì„¸ ì „ë¬¸ê°€ì˜ ê´€ì ì„ í†µí•©í•˜ì—¬ **SGR ìŠ¤íƒ€ì¼ì˜ ì‹¬ì¸µ ì „ëµ ë¦¬í¬íŠ¸**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            **ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, ê¹Šì´ ìˆëŠ” í†µì°°ê³¼ êµ¬ì²´ì ì¸ ë°ì´í„°ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.**
            **ê° ì„¹ì…˜ì€ ì¶©ë¶„íˆ ê¸¸ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš” (ìµœì†Œ A4 2ì¥ ë¶„ëŸ‰ì˜ ê¹Šì´).**
            ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ë©°, ì•„ë˜ í¬ë§·ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì„¸ìš”:

            # ğŸ“‘ SGR ë“œë¦¼íŒ€ ì „ëµ ë¦¬í¬íŠ¸: [ì£¼ì œ í‚¤ì›Œë“œ]

            ## 1. ğŸŒ ê±°ì‹œê²½ì œ ë° ì‹œì¥ í™˜ê²½ (Macro View)
            > *"ìˆ²ì„ ë¨¼ì € ë´…ë‹ˆë‹¤." - ê±°ì‹œê²½ì œ ì „ë¬¸ê°€*
            - **ê¸€ë¡œë²Œ ê²½ì œ íë¦„**: ê¸ˆë¦¬, í™˜ìœ¨, ìœ ê°€ ë“± ì£¼ìš” ê±°ì‹œ ì§€í‘œê°€ í•´ë‹¹ ì‚°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ìƒì„¸íˆ ì„œìˆ í•˜ì„¸ìš”.
            - **ì§€ì •í•™ì  ë¦¬ìŠ¤í¬**: ë¯¸ì¤‘ ê°ˆë“±, ê³µê¸‰ë§ ì´ìŠˆ ë“± ëŒ€ì™¸ ë³€ìˆ˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
            - **ì‹œì¥ ê¸°íšŒì™€ ìœ„í˜‘**: ê±°ì‹œì  ê´€ì ì—ì„œ ì‚¼ì„±ì—ê²Œ ë‹¤ê°€ì˜¤ëŠ” ê¸°íšŒ(Opportunity)ì™€ ìœ„í˜‘(Threat)ì„ ëª…í™•íˆ ì •ì˜í•˜ì„¸ìš”.

            ## 2. ğŸ”¬ ì‚°ì—… ë° ê¸°ìˆ  ë”¥ë‹¤ì´ë¸Œ (Tech Dive)
            > *"ê¸°ìˆ  ë””í…Œì¼ì— ì•…ë§ˆê°€ ìˆìŠµë‹ˆë‹¤." - ì‚°ì—… ê¸°ìˆ  ì „ë¬¸ê°€*
            - **ê²½ìŸì‚¬ ë™í–¥ ë¶„ì„**: TSMC, Intel, SKí•˜ì´ë‹‰ìŠ¤ ë“± ê²½ìŸì‚¬ì˜ ìµœê·¼ í–‰ë³´ì™€ ê¸°ìˆ  ê²©ì°¨ë¥¼ ìƒì„¸íˆ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”.
            - **ê¸°ìˆ  íŠ¸ë Œë“œ ì‹¬ì¸µ ë¶„ì„**: HBM, GAA, Advanced Packaging ë“± í•µì‹¬ ê¸°ìˆ ì˜ í˜„í™©ê³¼ ì „ë§ì„ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜/ìŠ¤í™ê³¼ í•¨ê»˜ ì„œìˆ í•˜ì„¸ìš”.
            - **ìˆ˜ìœ¨ ë° ìƒì‚° ì´ìŠˆ**: í˜„ì¬ ì œê¸°ë˜ê³  ìˆëŠ” ê¸°ìˆ ì  ë‚œì œì™€ í•´ê²° ë°©ì•ˆì„ ì „ë¬¸ê°€ì  ì‹œê°ì—ì„œ ì§„ë‹¨í•˜ì„¸ìš”.

            ## 3. ğŸš€ SGR ì „ëµ ì œì–¸ (Action Plan)
            > *"ê·¸ë˜ì„œ, ë‹¹ì¥ ë¬´ì—‡ì„ í•´ì•¼ í•©ë‹ˆê¹Œ?" - ì „ëµ ì»¨ì„¤í„´íŠ¸*
            - **Short-term Action (1ë…„ ë‚´)**: ë‹¹ì¥ ì‹¤í–‰í•´ì•¼ í•  êµ¬ì²´ì ì¸ ê³¼ì œ (ì˜ˆ: íŠ¹ì • ì¥ë¹„ ì¡°ê¸° ë„ì…, ê³ ê°ì‚¬ ë‹¤ë³€í™” ì „ëµ)ë¥¼ 3ê°€ì§€ ì´ìƒ ì œì•ˆí•˜ì„¸ìš”.
            - **Mid-to-Long-term Strategy (3ë…„ í›„)**: ë¯¸ë˜ ì‹œì¥ ì„ ì ì„ ìœ„í•œ ì¤‘ì¥ê¸° ë¡œë“œë§µ (ì˜ˆ: ì°¨ì„¸ëŒ€ R&D íˆ¬ì, M&A íƒ€ê²Ÿ)ì„ ì œì‹œí•˜ì„¸ìš”.
            - **Risk Management**: ì˜ˆìƒë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë³„ ëŒ€ì‘ ì „ëµ(Contingency Plan)ì„ í¬í•¨í•˜ì„¸ìš”.

            ## âš¡ Executive Summary (ê²½ì˜ì§„ ë¸Œë¦¬í•‘)
            (ë°”ìœ ì„ì›ì§„ì„ ìœ„í•œ 3ë¬¸ì¥ ì´ë‚´ì˜ í•µì‹¬ ê²°ë¡  ìš”ì•½)

            ## ğŸ“š ì£¼ìš” ì°¸ì¡° ì›ë¬¸ (Key Source Lists)
            - (ë³¸ ë¦¬í¬íŠ¸ ì‘ì„±ì— ê²°ì •ì ì¸ ê·¼ê±°ê°€ ëœ ê¸°ì‚¬ ì œëª©ê³¼ ì¶œì²˜ë¥¼ 3~5ê°œ ë‚˜ì—´í•˜ì„¸ìš”. ì˜ˆ: *"Samsung's 3nm strategy shift" - TechCrunch*)

            ---

            ---
            **[ë¶„ì„ ê¸°ì¤€ (Analysis Criteria)]**
            1. **Sentiment Score (-100 ~ 100)**: ì‹œì¥ì˜ ê°ì„± (Market Sentiment)
            - -100 ~ -50: ë§¤ìš° ë¶€ì •ì  (Bearish)
            - -49 ~ 49: ì¤‘ë¦½ (Neutral)
            - 50 ~ 100: ë§¤ìš° ê¸ì •ì  (Bullish)
            
            2. **Breakdown (ê¸ì • vs ë¶€ì • ë¹„ì¤‘)**
            - ê¸°ì‚¬ ë‚´ìš© ì „ì²´ë¥¼ 100ìœ¼ë¡œ ë´¤ì„ ë•Œ, ê¸ì •ì  ë‰˜ì•™ìŠ¤ì™€ ë¶€ì •ì  ë‰˜ì•™ìŠ¤ì˜ ë¹„ìœ¨
            
            3. **Bias Score (0~100)**: ê¸°ì‚¬ì˜ í¸í–¥ì„± (Media Bias)
            - 0~30: ë§¤ìš° ì¤‘ë¦½ì /ê°ê´€ì  (Fact-based)
            - 31~70: ë‹¤ì†Œ í¸í–¥ë¨ (Opinionated)
            - 71~100: ë§¤ìš° í¸í–¥ë¨/ì„ ë™ì  (Highly Biased)

            ---
            **[System Instruction: Output JSON Data]**
            ë¦¬í¬íŠ¸ ì‘ì„±ì´ ëë‚œ í›„, ë°˜ë“œì‹œ ë§¨ ë§ˆì§€ë§‰ ì¤„ì— ì•„ë˜ í˜•ì‹ìœ¼ë¡œ **JSON ë°ì´í„° í•˜ë‚˜ë§Œ** ì¶”ê°€í•˜ì„¸ìš”.
            ì´ ë°ì´í„°ëŠ” ì‹œê°í™”ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
            
            [[JSON_START]]
            {{
                "sentiment_score": 75,
                "sentiment_label": "Bullish (ê°•ì„¸)",
                "positivity_ratio": 80,
                "negativity_ratio": 20,
                "bias_score": 25,
                "bias_label": "Neutral (ì¤‘ë¦½ì )",
                "summary_reason": "AI ë°˜ë„ì²´ ìˆ˜ìš” í­ì¦ìœ¼ë¡œ ì¸í•œ ì‹¤ì  ê°œì„  ê¸°ëŒ€ê° ë°˜ì˜.",
                "positive_drivers": [
                    "ì—”ë¹„ë””ì•„ H200 ì£¼ë¬¸ëŸ‰ 3ë°° ì¦ê°€",
                    "ì‚¼ì„±ì „ìì˜ 3ë‚˜ë…¸ ìˆ˜ìœ¨ ê°œì„  ì†Œì‹",
                    "ë¯¸êµ­ ë“± ì£¼ìš”êµ­ì˜ ë°˜ë„ì²´ ì§€ì›ê¸ˆ í™•ëŒ€"
                ],
                "negative_risks": [
                    "ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ë¡œ ì¸í•œ ê³µê¸‰ë§ ë¶ˆì•ˆ",
                    "ì›ìì¬ ê°€ê²© ìƒìŠ¹ ì••ë°•"
                ]
            }}
            [[JSON_END]]
            """
            
            prompt = PromptTemplate(
                input_variables=["news_content"],
                template=template,
            )
            
            # Adjust context window based on model capability
            formatted_prompt = prompt.format(news_content=news_text[:current_context_limit])
            
            print(f"ğŸ¤– Trying model: {model_name}...")
            response = llm.invoke(formatted_prompt).content
            
            # If successful, return immediately
            return response

        except Exception as e:
            error_msg = str(e)
            print(f"âŒ Model {model_name} failed: {error_msg}")
            
            # Check specifically for Rate Limit (429) to try next model
            if "429" in error_msg or "Rate limit" in error_msg:
                continue
            # For other errors, also try next model just in case (optional, but safe)
            continue

    return "âŒ ëª¨ë“  AI ëª¨ë¸ì´ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (All models failed due to rate limits or errors)."
